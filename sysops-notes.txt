root aws account access IAM that can create
- user
- group
- role
- policy


IAM : identity access management

best practice to use individual accounts instead of root


ALB : application load balancer


accounts are not tied to one country but objects such as EC2 container, are


dynamic alias - one email address can have multiple accounts using alias


to create alias
1. services menu
2. security, identity & compliance
3. IAM


budget - set a budget and alert if going to reach limit
You will be notified when 
1) your actual spend reaches 85% 
2) your actual spend reaches 100% 
3) if your forecasted spend is expected to reach 100%


3 ways to log in to AWS
1. online console (website)
2. CLI
3. API


principal - a person/application that can make a request for an action on an AWS resource. part of RBAC I suppose

principal types
1. IAM user
2. IAM role
3. federated user
4. application



ARN : amazon resource name


two types of access control polcicy (allow/deny)
1. identity based
2. resource based



access control process

start

IAM principal

1. provide request context; info about the request
2. authentication; username/pw or access key ID and secret 
3. authorisation; use request context to determine if request is allow/deny



IAM user vs IAM role
- user is for long term access with permanent credentials
- role is for temporary access

role is safer than user. should minimise user
create user only when real human wants to access aws service


the "version" field within a custom trust policy json, is the version of syntax to use. It's a date value. Cannot anyhow change



IAM user group 
- a group of IAM user
- can set permissions for an entire group


permissions are ADDITIVE for user group and user
wherever there's explicit deny, it will always be followed regardless


can authenticate as a user, but not as a group

but can get permissions as a group


IAM user group is NOT a principal thats why cannot authenticate groups


actually IAM user is not recommended at all. Why?

- IAM user use permanent credentials. roles and IAM IC user do not

- IAM user must manage credentials for each account whereas IAM IC user can manage centrally

- IAM user is tied to single AWS acount. IAM IC user can connect to external active directory

- IAM user must be manually deleted if staff leaves. Since IAM IC user is connected to AD, can delete from AD and it will sync with AWS IAM IC




IAM user okay for smaller organisation




IAM role follows cloud native best practices because specific roles with limited access and expiry date are given to applications and services. Least privilege



1 IAM IC user can have access to several AWS accounts



VPC: virtual private cloud
- logical isolated portion of AWS within a region
- 1 region can have multiple VPC
- 1 VPC can have multiple availability zone
- 1 availability zone can have multiple subnet
- 1 subnet can have multiple EC2 instances

hierachy

Region
vpc
az
subnet
ec2 instance



within VPC is router and default gateway (internet gateway)


only way to interact with router is through the main route table


each VPC has its own CIDR block; aka its own local private subnet



S3 is outside VPC because its a public service
S3 is like google drive. It does not have querying like mongo or sql
S3 bucket names must be unique around the world, even other aws accounts


VPC endpoint is used to access AWS services that are public ie S3, through a private connection (doesnt touch internet)



API access to any AWS service ie dynamo, s3, ec2, all depend if using public connection or private connection. Both can work




terraform or other apps/services will use AWS Security Token Service (STS) to assume a role
it requires initial credentials which acts as the principal which tf authenticates with aws



========================================
IDEAL AWS security with terraform
========================================


create 1x IAM user account for perm credentials

keep credentials in vault

vault generates temp credentials for roles

terraform uses temp credentials and role

gitlab too



BUT HCP vault-secrets doesn't support aws secrets engine. The self hosted one does. so skip this for poc
TF itself will hold the perm credentials 


unused elastic IP will cost money. The auto default eip given to EC2 instances do not cost money



stateful vs stateless firewall
- stateful means when there is a rule that allows A talk to B, B talk to A is allowed by default
- stateless is opposite. If there's a rule that allows A ta B, B cannot talk to A by default



security group is stateful firewall
- security group can be applied to a group of instances within a subnet
- security groups apply to instance level
- only apply to instances that are within the group
- only has allow rules
- evaluate all rules flat
- deny all


network ACL is stateless firewall
- has allow and deny rules
- rules are in top down order
- NACL attach to subnets
- They apply to subnet level only
- applies to all instances within a subnet
- Only control traffic in and out of subnets
- does not control traffic WITHIN subnet
- only at the border of a subnet
- if one subnet talk to another, then there are 2 NACLs to check rules because each subnet has 1 NACL attached


ec2 host: the bare metal server running ec2


to attach instance to security group, add the security group under the instance definition


different kinds of ip addr, an ec2 instance can have
1. public ip
- standard internet ip
- no charge
- cannot be moved between instances
- lost when instance is stopped
- associated with a private ip
- used in public subnet only

2. private ip
- retained when instance stops
- used in public and private subnets


3. elastic ip 
- static public ip addr
- charged if not used
- associated with a private ip on the instance
- can move between instances



types of network interfaces
1. elastic network interface (ENI)
- cannot attach ENI from one AZ to another
- ENI can only attach to subnets within the same AZ
- can be used with any instance type

2. elastic network adapter (ENA)
- higher performance than ENI
- higher bandwith and lower latency
- only some instance support it


3. elastic fabric adapter (EFA)
- even higher performance
- usually for AIML use
- all instance support it





types of storage for ec2 instances

1. Elastic block store (EBS)
- persistant storage
- c drive on an ec2 instance could be stored on an ebs elsewhere, connected over network
- means same persistant storage can be used on different instance since ebs is separated
- ebs exist only within AZ

2. instance store
- not over network
- natively stored on ec2 host itself
- better performance
- non-persistant (ephemeral)
- only use when high performance needed or when the data has backups




amazon machine image (AMI)
- basically the OS image that the EC2 instance will use
- has an EBS snapshot linked to it (persistent storage)
- can predefine the image just like docker

stopping and starting an instance will mean that the physical storage used in baremetal is different each time


default username for ssh for t2.micro ec2 is:
For Amazon Linux 2 or the Amazon Linux AMI, the username is ec2-user.
For a CentOS AMI, the user name is centos.
For a Debian AMI, the user name is admin.
For a Fedora AMI, the user name is ec2-user or fedora.
For a RHEL AMI, the user name is ec2-user or root.
For a SUSE AMI, the user name is ec2-user or root.
For a Ubuntu AMI, the user name is ubuntu.
For an Oracle AMI, the user name is ec2-user.
For a Bitnami AMI, the user name is bitnami.
Otherwise, if ec2-user and root don't work, check with the AMI provider


you can change instance type (vertical scaling) and still retain data using EBS

instead of using ubuntu gparter, can use CLI
// Create a filesystem on the EBS volume 
sudo mkfs -t ext4 /dev/xvdf
// Create a mount point for the EBS volume 
sudo mkdir /data
//Mount the EBS volume to the mount point 
sudo mount /dev/xvdf /data
//Make the volume mount persistent
sudo nano /etc/fstab 
	== then add '/dev/xvdf /data ext4 defaults,nofail 0 2' and save the file



stopped instances (basically shutdown but dont delete instance)
- only for EBS backed instances
- EBS volume is chargeable
- instance is not chargeable
- data in RAM is lost
- instance will run on different host when start again
- private IPs retain but public change



hibernating instances (a bit like vmware paused)
- only for on-demand or reserved linux instances
- contents of RAM saved to EBS volume
- must enable hibernation before it is launched


rebooting instances
- does not affect billing
- DNS name, IPv4, IPv6 addr retained


retiring instances
- performed by AWS
- scheduled retirement date
- might be done early if host has irreparable failure


terminating instances
- EBS volume also deleted by default (can change)


recovering instances
- cloudwatch can be used to monitor instance status and repair
- occurs when host needs repair
- restore back to original state



Nitro:
- high performance hypervisor
- focused on performance, security, and innovation
- have specialised hardware ie
	= nitro VPC
	= nitro EBS
	= nitro instance storage
	= nitro security chip
- close to bare metal performance even with hypervisor
- ENA and EFA are based on nitro
- network can reach 100Gbps
- high performance compute



nitro enclave
- isolated hardened VM
- no persistant volume, interactive access, or external networking
- only authorised code can run. performs check
- integrates with key management service (KMS) for encryption
- good for protecting and processing sensitive data ie
	= personal data
	= healthcare
	= financial


event-driven architecture
1. user upload a file to static website, stored in S3
2. lambda function process file and store in S3


with serverless (lambda),
- no instances to manage
- no provision of hardware
- no provision of OS or software
- auto scaling


lambda charge you on compute time


SQS queue decoupling (Simple Queue Service)
- when the network layer receives high traffic, the high traffic needs to be sent to the app
- but problem is sometimes traffic so high that auto scaling kicks in and there is a short delay before network layer can send to app
- failure occurs when network layer cannot keep up with app
- so put a "proxy" to hold traffic
- this is achieved with SQS queue
- network layer and app do not talk to each other directly
- both talk to SQS queue
- PULL BASED


SNS decoupling (Simple Notification Service)
- event producer (publisher) sends message to SNS topic
- each subscriber to the SNS topic will receive the message in its native protocol ie text message will get SMS while web app get HTTP
- PUSHED BASED
- many-to-many messaging
- possible endpoints
	= SQS queue
	= lambda functions
	= SMS
	= webhooks
	= email
>> SNS + SQS fan out architecture
	-> one SNS topic has several SQS queues subscribed
	-> the SNS topic sends the same message to all SQS queues, but each SQS queue performs a different logic on the same message



file storage vs object storage
- file storage (EFS)
	= data stored in directories
	= nested directories
	= mounts to an OS

- object storage (S3)
	= stored in buckets
	= flat architecture. no nests
	= REST api



Elastic file system (EFS)
- can create EFS on vpc and attach instances from different AZ to the EFS
- can simultaneously connect many instances
- only for linux instances since using NFS
- if want to connect instances from other VPC, need to use peering
- can VPN to on prem clients
- can use IAM to authenticate access


FSx for windows
- similar to EFS but for windows
- can use AD to authenticate
- can VPN to on prem clients

FSx for Lustre
- high performance version
- works with S3 too
- S3 buckets are presented as files
- designed for
	= ML
	= video processing
- can VPN to on prem clients



Simple storage service (S3)
- bucket names must be unique globaly
- can define buckets per region
- unlimited storage
- can be any file type
- key:value style but no hierachy
- can create folders. they can nest
- buckets cannot nest
- bucket contents:
	= key
	= version ID
	= value
	= subresources
	= access control info

=================================
mount EFS to EC2 instance

from within an ec2 instance

0. mkdir ~/efs-mount-point
1. Install EFS utils sudo yum install -y amazon-efs-utils
2. Mount using the EFS mount helper sudo mount -t efs -o tls fs-0449ea760b768b5b4.efs.ap-southeast-1.amazonaws.com:/ ~/efs-mount-point


=================================



S3 data can also be encrypted at rest


accessing S3 buckets
1. use AWS API
	- dont make sense for web app

2. set up API gateway, and lambda function to process API requests
	- mongo atlas has a built in API gateway that can process API calls natively
	- S3 does not have native API gateway so must set up
		= first need to create a role for the lmabda function, that only has S3 access (doesn't need IAM auth because lambda is within AWS)
		= create API gateway that requires IAM authentication 
			( can use IAM role, but an IAM user has to supply credentials to this role )
		= create lambda function to take API call, and return the specified S3 object




Relational database service (RDS)
- SQL
- typically used to store transactions ie purchases from online store
- data can be encrypted at rest
- encryption uses key management service (KMS)
- uses several DB types
	= aurora
	= MySQL
	= oracle
	= MS SQL
	Aurora
	- SQL db created by aws
	- the native DB used by RDS
	- supposedly faster than MySQL and postgre

- RDS DB lives within a VPC too
- RDS DB can be given public ip
- security groups can be attached to RDS DB
- can also enable TLS for RDS DB encryption
- RDS DB data can also be encrypted at rest
- KMS is used to manage the encryption
- read replica will always have same encryption status as object in RDS DB
- KMS key must be from same region as RDS DB
- cannot restore an unencrypted backup or snapshot to an encrypted RDS DB



DynamoDB
- noSQL 



ElastiCache
- good for static data thats frequently accessed
- key/value store
- runs on EC2 instances
- high performance low latency
- can be used as a cache for RDS
- got different types of objects within elasticache
	= memcached
		> no data persistence
		> simple data type
		> no encryption
		> multi-threaded
		> no backups or snapshots
		> no replica
		> have data partitioning

	= Redis with cluster mode
		> have data persistance
		> complex data type
		> have encryption
		> have replica
		> not muli-threaded
		> automatic backup and manual snapshot
		> have data partitioning

	= Redis with NO cluster mode
		> have data persistance
		> complex data type
		> have encryption
		> have replica
		> not muli-threaded
		> automatic backup and manual snapshot
		> no data partitioning




cloudwatch
- aws observability
- automate responses to certain changes
- can source from both aws and on prem

	cloudwatch metrics
		> numerical data
		> to get system level metrics such as CPU and RAM usage, need to attach a cloudwatch agent to EC2 instance
		> can set custom metrics outside of aws, to cloudwatch
		> metrics exist within a region
		> all metrics must belong to a namespace

	cloudwatch alarms
		> monitor metrics and initiate actions
		> 2 types of alarms
			metric alarm: perform some actions based on one metric
			composite alarm: several alarms together, managed by rules
		> metric alarm states:
			OK - below threshold
			ALARM - above threshold
			INSUFFICIENT_DATA 

		> can work with EC2 auto scaling to define a threshold when to add more instances

	cloudwatch logs
		> system and app logs
		> by default, logs are not expired. can set retention policy
		> can be encyrpted with KMS
		> types of log objects
			log events: activity
			log streams: a sequence of log events that share the same source
			log groups: a group of log streams that share the same settings ie access control, retention
		> metric filters can be created to convert logs to metrics
			(ie logs show a lot of 404 and 200 HTTP status. use metric filter for 404 to see 404 count graph )

	cloudwatch events (replaced by event bridge)
		> stream of system events. can trigger actions too


	cloudwatch dimensions
		> a dimension is a name:value pair that is part of the identity of a metric
		> can assign multiple dimensions to a metric
		> similar to namespace but dimensions are used to categorise the metric

	cloudwatch statistics
		> aggregation of metric data over a period of time


	cloudwatch API actions (in exam)
	= GetMetricData
		+ retrieve up to 500 metrics in a single request

	= PutMetricData
		+ publish metric data to cloudwatch
		+ used for custom metrics

	= GetMetricStatistics
		+ get stats for specific metric
		+ max number of datapoints returned is 1440






to create an instance profile
1. create a role
2. associate role with instance profile
3. associate instance profile with ec2 instance




cloudwatch agent
- collect metrics and logs from EC2 instances and on-prem
- instead of manually using "free" command in amazon linux to output system memory usage, 
can use cloudwatch agent to natively draw system metrics such as
cpu usage, cpu time, memory etc




CloudTrail
- logs API activity. not performanced oriented. audit focused
- CloudTrail Trail can be limited to one region or all
- CloudTrail Trail logs any events to S3 indefinitely
- CloudTrail can trigger CloudWatch Events
- CloudTrail logs API activity for 90 days by default
- CloudTrail logs can trigger notification by subscribing to SNS topic
- types of CloudTrail Events
	= Management Events
		-> info on management operations performed on resources in an AWS account
		-> ie. Creating S3 bucket, creating IAM principals
		-> creating route table rules
	= Data Events
		-> info on RESOURCE operations performed on resources
		-> ie. S3 API calls such as GetObject, DeleteObject
		-> Lamba function calls
		-> DynamoDB calls
	= Insights Events
		-> identify and respond to sus API calls. Monitors Management events