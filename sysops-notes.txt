root aws account access IAM that can create
- user
- group
- role
- policy


IAM : identity access management

best practice to use individual accounts instead of root


ALB : application load balancer


accounts are not tied to one country but objects such as EC2 container, are


dynamic alias - one email address can have multiple accounts using alias


to create alias
1. services menu
2. security, identity & compliance
3. IAM


budget - set a budget and alert if going to reach limit
You will be notified when 
1) your actual spend reaches 85% 
2) your actual spend reaches 100% 
3) if your forecasted spend is expected to reach 100%


3 ways to log in to AWS
1. online console (website)
2. CLI
3. API


principal - a person/application that can make a request for an action on an AWS resource. part of RBAC I suppose

principal types
1. IAM user
2. IAM role
3. federated user
4. application



ARN : amazon resource name


two types of access control polcicy (allow/deny)
1. identity based
2. resource based



access control process

start

IAM principal

1. provide request context; info about the request
2. authentication; username/pw or access key ID and secret 
3. authorisation; use request context to determine if request is allow/deny



IAM user vs IAM role
- user is for long term access with permanent credentials
- role is for temporary access

role is safer than user. should minimise user
create user only when real human wants to access aws service


the "version" field within a custom trust policy json, is the version of syntax to use. It's a date value. Cannot anyhow change



IAM user group 
- a group of IAM user
- can set permissions for an entire group


permissions are ADDITIVE for user group and user
wherever there's explicit deny, it will always be followed regardless


can authenticate as a user, but not as a group

but can get permissions as a group


IAM user group is NOT a principal thats why cannot authenticate groups


actually IAM user is not recommended at all. Why?

- IAM user use permanent credentials. roles and IAM IC user do not

- IAM user must manage credentials for each account whereas IAM IC user can manage centrally

- IAM user is tied to single AWS acount. IAM IC user can connect to external active directory

- IAM user must be manually deleted if staff leaves. Since IAM IC user is connected to AD, can delete from AD and it will sync with AWS IAM IC




IAM user okay for smaller organisation




IAM role follows cloud native best practices because specific roles with limited access and expiry date are given to applications and services. Least privilege



1 IAM IC user can have access to several AWS accounts



VPC: virtual private cloud
- logical isolated portion of AWS within a region
- 1 region can have multiple VPC
- 1 VPC can have multiple availability zone
- 1 availability zone can have multiple subnet
- 1 subnet can have multiple EC2 instances

hierachy

Region
vpc
az
subnet
ec2 instance



within VPC is router and default gateway (internet gateway)


only way to interact with router is through the main route table


each VPC has its own CIDR block; aka its own local private subnet



S3 is outside VPC because its a public service
S3 is like google drive. It does not have querying like mongo or sql
S3 bucket names must be unique around the world, even other aws accounts


VPC endpoint is used to access AWS services that are public ie S3, through a private connection (doesnt touch internet)



API access to any AWS service ie dynamo, s3, ec2, all depend if using public connection or private connection. Both can work




terraform or other apps/services will use AWS Security Token Service (STS) to assume a role
it requires initial credentials which acts as the principal which tf authenticates with aws



========================================
IDEAL AWS security with terraform
========================================


create 1x IAM user account for perm credentials

keep credentials in vault

vault generates temp credentials for roles

terraform uses temp credentials and role

gitlab too



BUT HCP vault-secrets doesn't support aws secrets engine. The self hosted one does. so skip this for poc
TF itself will hold the perm credentials 


unused elastic IP will cost money. The auto default eip given to EC2 instances do not cost money



stateful vs stateless firewall
- stateful means when there is a rule that allows A talk to B, B talk to A is allowed by default
- stateless is opposite. If there's a rule that allows A ta B, B cannot talk to A by default



security group is stateful firewall
- security group can be applied to a group of instances within a subnet
- security groups apply to instance level
- only apply to instances that are within the group
- only has allow rules
- evaluate all rules flat
- deny all


network ACL is stateless firewall
- has allow and deny rules
- rules are in top down order
- NACL attach to subnets
- They apply to subnet level only
- applies to all instances within a subnet
- Only control traffic in and out of subnets
- does not control traffic WITHIN subnet
- only at the border of a subnet
- if one subnet talk to another, then there are 2 NACLs to check rules because each subnet has 1 NACL attached


ec2 host: the bare metal server running ec2


to attach instance to security group, add the security group under the instance definition


different kinds of ip addr, an ec2 instance can have
1. public ip
- standard internet ip
- no charge
- cannot be moved between instances
- lost when instance is stopped
- associated with a private ip
- used in public subnet only

2. private ip
- retained when instance stops
- used in public and private subnets


3. elastic ip 
- static public ip addr
- charged if not used
- associated with a private ip on the instance
- can move between instances



types of network interfaces
1. elastic network interface (ENI)
- cannot attach ENI from one AZ to another
- ENI can only attach to subnets within the same AZ
- can be used with any instance type

2. elastic network adapter (ENA)
- higher performance than ENI
- higher bandwith and lower latency
- only some instance support it


3. elastic fabric adapter (EFA)
- even higher performance
- usually for AIML use
- all instance support it





types of storage for ec2 instances

1. Elastic block store (EBS)
- persistant storage
- c drive on an ec2 instance could be stored on an ebs elsewhere, connected over network
- means same persistant storage can be used on different instance since ebs is separated
- ebs exist only within AZ
- can create EBS snapshots just like VMware. Incremental snapshots. Can create an AMI from a snapshot
- can start a new instance in another AZ that's using a snapshot (not EBS volume) that was created in another AZ

2. instance store
- not over network
- natively stored on ec2 host itself
- better performance
- non-persistant (ephemeral)
- only use when high performance needed or when the data has backups




amazon machine image (AMI)
- basically the OS image that the EC2 instance will use
- has an EBS snapshot linked to it (persistent storage)
- can predefine the image just like docker
- can make your AMI publicly available or private
- cannot delete AMI snapshot while an instance is still running, using that AMI snapshot

stopping and starting an instance will mean that the physical storage used in baremetal is different each time


default username for ssh for t2.micro ec2 is:
For Amazon Linux 2 or the Amazon Linux AMI, the username is ec2-user.
For a CentOS AMI, the user name is centos.
For a Debian AMI, the user name is admin.
For a Fedora AMI, the user name is ec2-user or fedora.
For a RHEL AMI, the user name is ec2-user or root.
For a SUSE AMI, the user name is ec2-user or root.
For a Ubuntu AMI, the user name is ubuntu.
For an Oracle AMI, the user name is ec2-user.
For a Bitnami AMI, the user name is bitnami.
Otherwise, if ec2-user and root don't work, check with the AMI provider


you can change instance type (vertical scaling) and still retain data using EBS

instead of using ubuntu gparter, can use CLI
// Create a filesystem on the EBS volume 
sudo mkfs -t ext4 /dev/xvdf
// Create a mount point for the EBS volume 
sudo mkdir /data
//Mount the EBS volume to the mount point 
sudo mount /dev/xvdf /data
//Make the volume mount persistent
sudo nano /etc/fstab 
	== then add '/dev/xvdf /data ext4 defaults,nofail 0 2' and save the file



stopped instances (basically shutdown but dont delete instance)
- only for EBS backed instances
- EBS volume is chargeable
- instance is not chargeable
- data in RAM is lost
- instance will run on different host when start again
- private IPs retain but public change



hibernating instances (a bit like vmware paused)
- only for on-demand or reserved linux instances
- contents of RAM saved to EBS volume
- must enable hibernation before it is launched


rebooting instances
- does not affect billing
- DNS name, IPv4, IPv6 addr retained


retiring instances
- performed by AWS
- scheduled retirement date
- might be done early if host has irreparable failure


terminating instances
- EBS volume also deleted by default (can change)


recovering instances
- cloudwatch can be used to monitor instance status and repair
- occurs when host needs repair
- restore back to original state



Nitro:
- high performance hypervisor
- focused on performance, security, and innovation
- have specialised hardware ie
	= nitro VPC
	= nitro EBS
	= nitro instance storage
	= nitro security chip
- close to bare metal performance even with hypervisor
- ENA and EFA are based on nitro
- network can reach 100Gbps
- high performance compute



nitro enclave
- isolated hardened VM
- no persistant volume, interactive access, or external networking
- only authorised code can run. performs check
- integrates with key management service (KMS) for encryption
- good for protecting and processing sensitive data ie
	= personal data
	= healthcare
	= financial


event-driven architecture
1. user upload a file to static website, stored in S3
2. lambda function process file and store in S3


with serverless (lambda),
- no instances to manage
- no provision of hardware
- no provision of OS or software
- auto scaling


lambda charge you on compute time


SQS queue decoupling (Simple Queue Service)
- when the network layer receives high traffic, the high traffic needs to be sent to the app
- but problem is sometimes traffic so high that auto scaling kicks in and there is a short delay before network layer can send to app
- failure occurs when network layer cannot keep up with app
- so put a "proxy" to hold traffic
- this is achieved with SQS queue
- network layer and app do not talk to each other directly
- both talk to SQS queue
- PULL BASED


SNS decoupling (Simple Notification Service)
- event producer (publisher) sends message to SNS topic
- each subscriber to the SNS topic will receive the message in its native protocol ie text message will get SMS while web app get HTTP
- PUSHED BASED
- many-to-many messaging
- possible endpoints
	= SQS queue
	= lambda functions
	= SMS
	= webhooks
	= email
>> SNS + SQS fan out architecture
	-> one SNS topic has several SQS queues subscribed
	-> the SNS topic sends the same message to all SQS queues, but each SQS queue performs a different logic on the same message



file storage vs object storage
- file storage (EFS)
	= data stored in directories
	= nested directories
	= mounts to an OS

- object storage (S3)
	= stored in buckets
	= flat architecture. no nests
	= REST api



Elastic file system (EFS)
- can create EFS on vpc and attach instances from different AZ to the EFS
- can simultaneously connect many instances
- only for linux instances since using NFS
- if want to connect instances from other VPC, need to use peering
- can VPN to on prem clients
- can use IAM to authenticate access


FSx for windows
- similar to EFS but for windows
- can use AD to authenticate
- can VPN to on prem clients

FSx for Lustre
- high performance version
- works with S3 too
- S3 buckets are presented as files
- designed for
	= ML
	= video processing
- can VPN to on prem clients



Simple storage service (S3)
- bucket names must be unique globaly
- can define buckets per region
- unlimited storage
- can be any file type
- key:value style but no hierachy
- can create folders. they can nest
- buckets cannot nest
- bucket contents:
	= key
	= version ID
	= value
	= subresources
	= access control info

=================================
mount EFS to EC2 instance

from within an ec2 instance

0. mkdir ~/efs-mount-point
1. Install EFS utils sudo yum install -y amazon-efs-utils
2. Mount using the EFS mount helper sudo mount -t efs -o tls fs-0449ea760b768b5b4.efs.ap-southeast-1.amazonaws.com:/ ~/efs-mount-point


=================================



S3 data can also be encrypted at rest


accessing S3 buckets
1. use AWS API
	- dont make sense for web app

2. set up API gateway, and lambda function to process API requests
	- mongo atlas has a built in API gateway that can process API calls natively
	- S3 does not have native API gateway so must set up
		= first need to create a role for the lmabda function, that only has S3 access (doesn't need IAM auth because lambda is within AWS)
		= create API gateway that requires IAM authentication 
			( can use IAM role, but an IAM user has to supply credentials to this role )
		= create lambda function to take API call, and return the specified S3 object




Relational database service (RDS)
- SQL
- typically used to store transactions ie purchases from online store
- data can be encrypted at rest
- encryption uses key management service (KMS)
- uses several DB types
	= aurora
	= MySQL
	= oracle
	= MS SQL
	Aurora
	- SQL db created by aws
	- the native DB used by RDS
	- supposedly faster than MySQL and postgre

- RDS DB lives within a VPC too
- RDS DB can be given public ip
- security groups can be attached to RDS DB
- can also enable TLS for RDS DB encryption
- RDS DB data can also be encrypted at rest
- KMS is used to manage the encryption
- read replica will always have same encryption status as object in RDS DB
- KMS key must be from same region as RDS DB
- cannot restore an unencrypted backup or snapshot to an encrypted RDS DB



DynamoDB
- noSQL 



ElastiCache
- good for static data thats frequently accessed
- key/value store
- runs on EC2 instances
- high performance low latency
- can be used as a cache for RDS
- got different types of objects within elasticache
	= memcached
		> no data persistence
		> simple data type
		> no encryption
		> multi-threaded
		> no backups or snapshots
		> no replica
		> have data partitioning

	= Redis with cluster mode
		> have data persistance
		> complex data type
		> have encryption
		> have replica
		> not muli-threaded
		> automatic backup and manual snapshot
		> have data partitioning

	= Redis with NO cluster mode
		> have data persistance
		> complex data type
		> have encryption
		> have replica
		> not muli-threaded
		> automatic backup and manual snapshot
		> no data partitioning




cloudwatch
- aws observability
- automate responses to certain changes
- can source from both aws and on prem

	cloudwatch metrics
		> numerical data
		> to get system level metrics such as CPU and RAM usage, need to attach a cloudwatch agent to EC2 instance
		> can set custom metrics outside of aws, to cloudwatch
		> metrics exist within a region
		> all metrics must belong to a namespace

	cloudwatch alarms
		> monitor metrics and initiate actions
		> 2 types of alarms
			metric alarm: perform some actions based on one metric
			composite alarm: several alarms together, managed by rules
		> metric alarm states:
			OK - below threshold
			ALARM - above threshold
			INSUFFICIENT_DATA 

		> can work with EC2 auto scaling to define a threshold when to add more instances

	cloudwatch logs
		> system and app logs
		> by default, logs are not expired. can set retention policy
		> can be encyrpted with KMS
		> types of log objects
			log events: activity
			log streams: a sequence of log events that share the same source
			log groups: a group of log streams that share the same settings ie access control, retention
		> metric filters can be created to convert logs to metrics
			(ie logs show a lot of 404 and 200 HTTP status. use metric filter for 404 to see 404 count graph )

	cloudwatch events (replaced by event bridge)
		> stream of system events. can trigger actions too


	cloudwatch dimensions
		> a dimension is a name:value pair that is part of the identity of a metric
		> can assign multiple dimensions to a metric
		> similar to namespace but dimensions are used to categorise the metric

	cloudwatch statistics
		> aggregation of metric data over a period of time


	cloudwatch API actions (in exam)
	= GetMetricData
		+ retrieve up to 500 metrics in a single request

	= PutMetricData
		+ publish metric data to cloudwatch
		+ used for custom metrics

	= GetMetricStatistics
		+ get stats for specific metric
		+ max number of datapoints returned is 1440






to create an instance profile
1. create a role
2. associate role with instance profile
3. associate instance profile with ec2 instance




cloudwatch agent
- collect metrics and logs from EC2 instances and on-prem
- instead of manually using "free" command in amazon linux to output system memory usage, 
can use cloudwatch agent to natively draw system metrics such as
cpu usage, cpu time, memory etc




CloudTrail
- logs API activity. not performanced oriented. audit focused
- CloudTrail Trail can be limited to one region or all
- CloudTrail Trail logs any events to S3 indefinitely
- CloudTrail can trigger CloudWatch Events
- CloudTrail logs API activity for 90 days by default
- CloudTrail logs can trigger notification by subscribing to SNS topic
- types of CloudTrail Events
	= Management Events
		-> info on management operations performed on resources in an AWS account
		-> ie. Creating S3 bucket, creating IAM principals
		-> creating route table rules
	= Data Events
		-> info on RESOURCE operations performed on resources
		-> ie. S3 API calls such as GetObject, DeleteObject
		-> Lamba function calls
		-> DynamoDB calls
	= Insights Events
		-> identify and respond to sus API calls. Monitors Management events



EventBridge (previously known as Cloudwatch Events)

- Event sources:
	= AWS services
	= custom apps

- event sources send data to event bus
- event bus send data to targets, according to rules
- can set a schedule to perform a lambda function
	= ie. tmr at 2pm run this function



AWS config
- define rules as to how the infrastructure should be like
- ie must have 2x ec2 instance and 1x RDS
- if dont have such infra, AWS config can alert SNS,
or send to event bridge to perform something through lambda
- or use systems manager automation to perform remediation



Systems Manager
- manage EC2 instances, applications, and even VMs from other cloud providers, and self hosted VMs
- must install the SSM agent
- need IAM role attached to the SSM agent
- all actions can be recorded by CloudTrail


	systems manager automation
	- YAML IAC config files
	- a bit like terraform
	- can automatically perform remediation, and not just alert

	systems manager run command
	- same YAML IAC config file, but instead of running an operation, run a command

	systems manager inventory
	- an inventory of all managed services
	- dashboard

	systems manager patch manager
	- patch baselines with rules
	- patch EC2 instances and on prem VMs
	- set patch time

	systems manager compliance
	- scan instances for patch / config compliance
	- can aggregate data from multiple AWS accounts and regions
	- can create custom compliance rules for own custom objects, and not just EC2 instance OS update

	systems manager session manager
	- remotely interact instances without SSH, or logging in to them
	- can store session logs in S3, and output to CloudWatch Logs
	- don't need to open port for ssh

	systems manager parameter store
	- secret storage
	- key:value
	- no auto rotation of keys (secrets manager does)


Personal Health Dashboard
- provide personalised proactive alerts and remediation advice when AWS has events that impact the user
- ie AWS scheduled maintenance


Service Health Dashboard
- status of all services
- even services the user doesn't use
- no proactive notification of scheduled activties


Auto scaling
- EC2 instances, EC2 spot fleets, DynamoDB, Aurora
- works with CloudWatch
- horizontal scaling
- launch and terminate instances dynamically
- scale based on time or performance
- auto scaling group: just a group of objects that share the same auto scaling settings (ASG)
- Health Checks
	EC2 status checks
	ELB + EC2 status checks
- Health Check grace period: how long to wait before checking health status
- ASG can cover sevceral AZs or subnets
- ASG require a launch template

- types of auto scaling
	Dynamic: automatically scale based on demand
	Predictive: uses AIML to predict
	Scheduled: based on time
- types of auto scaling policy
	= Target tracking scaling
		-> scale based on current CloudWatch metrics values
	= Step scaling
		-> scale in steps, based on severity of the alarm
	= Simple Scaling
		-> scale based on a single static source
		-> Cooldown: used with simple scaling policy to add buffer time before scaling occurs. Default is 5mins
	= Termination policy
		-> set which instance to terminate first when scaling occurs
	= Termination protection policy
		-> set which instance NOT to terminate regardless of scaling

- auto scaling settings
	= lifecycle hooks
		-> perform some actions as instances are scaled


Elastic Load Balancing (ELB)
- can cover several AZ
- Application load balancer 
	= request level of OSI model (layer7)
	= HTTP, HTTPS
	= good with docker and lambda targets
- Network load balancer
	= connection level of OSI model (layer4)
	= TCP, UDP
	= higher performance


storing session state
option1: store session data in dynamoDB so when user is redirected to another instance while logged in to the web app, no need to sign in again

option2: elasticache

option3: cookies (locally stored on client side). used with sticky sessions


RDS automated backups
- set backup window
- when backup occurs, snapshot of the DB is sent to S3
- transaction logs are also sent every 5 mins
- if want to restore a backup, a new DB instance is created
- can replicate backups to other regions
- brief suspension of I/O during snapshot taking

RDS manual backups
- snapshot don't expire
- brief suspension of I/O during snapshot taking


RDS maintenance window
- by default weekly


Aurora backups
- default backup retention period is 1 day
- cannot disable automated backups
- can use AWS Backup to manage backups of Aurora clusters
- manual snapshots do not expire
- can copy snapshots across regions
 

RDS disaster recovery (DR)
- RDS Primary: the main db
- RDS Standby: a replica of the primary db in another AZ (synchronous replication)
- RDS Read Replica: offload read queries to a replica (asynchronous replication). Used when read traffic high


RDS Fault Tolerance and Replicas
- Fault Tolerance works across 3 AZs
- 1 Primary in 1 AZ, and the other 2 AZs have replicas. But only one single logical volume
- Replica only take in read requests. Write requests are handled by Primary
- Can promote Replica to a Primary. Can use auto-scaling to horizontally scale more replicas
- Replicas are within a region
- if want to perform cross region replication, need to use Aurora MySQL (asynchronous)


Aurora Global Database
- cross region replication without using MySQL
- uses Aurora storage layer
- better performance


Aurora Multi-Master
- many Primary DB. Only for MySQL
- Single region only


Aurora Serverless
- DB scales automatically based on usage
- Aurora Capacity Unit (ACU), the PC running the Aurora db
- can set how many ACU to use
- good when unsure of the db use case, so don't want to set in stone the db architecture yet


Route 53
- DNS service
	= can have public and private DNS
- Domain registration
- Health check of EC2 instances
- Hosted Zones
	= group of domains and subdomain
	= can associate a Hosted Zone to a VPC
- can migrate from other DNS to Route 53
- can migrate a Hosted Zone to another AWS account
- can associate a Hosted Zone with a VPC from another AWS account
- Routing Policy:
	= Simple
		-> basic DNS

	= Weighted
		-> one domain may have multiple IPs (several regions to serve the website)
		-> set weights so that ie 60% traffic goes to region 1, 40% traffic goes to region 2

	= Latency
		-> region with lowest latency will respond to DNS query

	= Failover
		-> Set health check on primary Route 53 Hosted Zone
		-> if fail, route to secondary

	= Geolocation
		-> Set specific region regardless of latency

	= MultiValue
		-> one domain may have multiple IPs (several regions to serve the website)
		-> Randomly pick which Hosted Zone to serve the DNS query

	= IP based
		-> route basedo on subnet and CIDR
		



Recovery Point Objective (RPO)
- measurement of the amount of data that can be lost
- measured in seconds minutes or hours
- 2hr RPO means must back up every 2hours
- to achieve milliseconds of RPO, use synchronous replication (1-60ms)
- to achieve seconds of RPO, use asynchronous replication (1-60s)
- to achieve minutes of RPO, use snapshots, S3 backups (1-60mins)
- to achieve hours of RPO, use on prem off site backups (1- ~ hours)


Recovery Time Objective (RTO)
- measurement of time it takes to restore
- measured in seconds minutes or hours
- to achieve milliseconds of RTO, use fault tolerance
- to achieve seconds of RTO, use load balancer, auto scaling
- to achieve minutes of RTO, use cloud cross site recovery
- to achieve hours of RTO, use on prem recovery


Disaster Recovery strategies (DR)

- backup and restore
	= low priority workloads
	= EBS snapshots, copy of file systems
	= copies of data
	= low cost

- Pilot Light
	= data is replicated
	= redundant instances
	= services may be idle
	= medium cost

- Warm Standby
	= business critical workloads
	= nodes always running
	= horizontal/vertical scaling
	= high cost

- Multi Site
	= data across multiple datacenters
	= data across regions
	= zero downtime
	= zero data loss
	= highest cost




AWS Backup
- automated data protection service
- centralised backup management for
	= EC2, EBS, EFS, DynamoDB, RDS
- Policy based
- Encrypted with KMS
- ransomware protectection


Data Lifecycle Manager (DLM)
- automates the management of EBS snapshots and EBS-backed AMIs
- create, retain, delete
- reduce cost by automatically delete old snapshots




Durability
- protect against data loss
- protect against data corruption
- S3 offers 11x 9 durability



Availability
- amount of time data is accessible


S3 Storage classes
> all guarantee 9x 11 durability

[top is most expensive, lowest latency, good for frequent access]

- S3 standard

- S3 intelligent Tiering

- S3 Standard IA (infrequent access)

- S3 One Zone IA

- S3 Glacier IR (instant retrieval)

- S3 Glacier FR

- S3 Clacier Deep Archive


S3 Replication
- Same-Region Replication (SSR): make a replica bucket within the same region


S3 Encryption
- can use S3 managed keys
- can also use KMS
- data is encrypted at risk. Decrypted when retrieved



S3 lifecycle management

two types of actions
- transition actions
	-> when objects move to another S3 storage class

- expiration actions
	-> when objects expire



CloudFormation
- basically terraform on AWS
- components
** basically template is the blueprint of a house while stack is the house
	= Templates
		-> the files themselves. can be YAML or JSON
		-> Intrinsic functions (built in fn in Templates)
			+ Ref
				return ID or Name
			+ :GetAtt
				return ARN or IP adr
			+ :FindInMap
				return value within a key:pair mapping
			+ AWS::Serverless
				specified which version of AWS serverless application model (SAM) to use
			+ AWS::Include
				reference code from another YAML

		-> mappings can use region name as key

	= Stacks
		->  the environment described by the template
		->  rollback upon failure is default

	= StackSets
		-> manage stack that spans across multiple accounts and regions

	= Nested Stacks
		-> a "helper function" that a stack can reference
		-> Like a standard template for info that is repeated

	= Change Sets
		-> summary of proposed changes to verify before they apply



CloudFormation Helper Scripts

cf-init
AWS::CloudFormation::Init
- can define what packages to install as EC2 instance is initialised
- can define what files to be created
- can define what services to enable
- kinda replaces AMI
- logs to /var/log/cfn-init.log

cfn-signal
- signal the status of EC2 instance creation
- can set the pipeline to wait until all instances are ready before proceding
- used with CloudFormation CreationPolicy

***in exam
	-> check if the scripts are included
	-> check if the commands have run successfully



CloudFormation CreationPolicy
- used with both CloudFormation Helper Scripts
- CreationPolicy is the policy defined in cloudformation file, to indicate wait for instance initialisation
- cfn-signal is the command run within an instance to send the signal to the cloudformation server

- supported resource types that CreationPolicy can be associated to,
	= AutoScalingGroup
	= Instance
	= WaitCondition

CloudFormation DeletionPolicy
- backup a resource when its stack is deleted
- all resources are deleted by default
- 3 parameters can be set. Either one
	= Retain
	= Snapshot
	= Delete

CloudFormation DependsOn
- specify that the creation of a resource follows another
- when a DepndsOn attirbute is added to a resource, that resource is only created AFTER the creation of the resource specified in the DependsOn Attibute
- if Resource A depends on Resource B, B must be created first

CloudFormation WaitCondition
- a bit like if statement. if xxx is performed then the resource can be created


CloudFormation UpdatePolicy
- specified update settings for
	= AWS::AutoScaling::ASG
	= AWS::ElastiCache::ReplicationGroup
	= AWS::Elasticsearch::Domain
	= AWS::Lambda::Alias
- can be used to enable rolling updates
- 

CloudFormation UpdateReplacePolicy
- retain or backup the existing physical instance of a resource when it is being updated



CloudFormation Rollback
- by default, everything will be deleted upon failure
- is applied to a whole stack
- OnFailure options:
	= DO_NOTHING
	= ROLLBACK
		- go back to previous state
	= DELETE
		- delete entire stack

CloudFormation UPDATE_ROLLBACK_FAILED
- stack update failure
- defualt rollback
- cannot update a stack while in this state
- it needs to go back to original state then can update



Resource Access Manager (RAM)
- share resources across AWS accounts, but within AWS organisations
- share between IAM roles and users
- can clickops, CLI, or API


Elastic Beanstalk
- automatically create web app environments
- PaaS
- similar to CloudFormation but specifically for web app env
- more for management than creation
- can launch and manage EC2 instaces, or even Docker running on ECS
- a cloudformation stack is automatically created
- Elastic Beanstalk environment
	= a set of resources provided by Elastic Beanstalk for a certain version of the web app
	= can set prod environment, and testing environment

- can deploy 2 types
	= web servers
	= workers
		-> web servers that specifically listen for messages from SQS then perform an action
		-> used for long running tasks

- Elastic Beanstalk process:
	= user access website
	= web server serves user
	= web server sends task request to SQS queue
	= worker polls SQS queue and waits for task request (pull based)

	-> in this example, SQS enables asynchronous processing
	-> async means user does not need to wait for service to process before moving forward
	-> SQS ensures that the worker is never overloaded
	-> SQS adds a layer of fault tolerance. should the worker go down, the request is still at the SQS queue so it can pick up where it left off, when the worker is back online



Elastic Beanstalk update types
- All at once
	= deploys new app the all instances at the same time
	= all instances go down at the same time too
	= fastest deployment but high downtime

- Rolling
	= update existing instances one by one until done
	= Long deployment time

- immutable
	= launch new instances with the new app inside, in a new ASG then when new ones are ready, old ones are turned off. the old and new serve traffic
	= zero downtime
	= higher cost
	= long deployment
	
- blue green
	= similar to immutable, but entirely new environment is created. When new instances are ready, all traffic is immediately swapped over


Elastic Beanstalk pricing
- it doesn't incur extra costs to use elastic beanstalk
- it charges only for the underlying services used such as EC2, S3, ASG etc..



OpsWorks
- managed service for Chef and Puppet
- chef and puppet are similar to ansible
- terraform provision cloud resources while ansible ensures the current state matches the desired state (configuration management)



Storage Gateway
- a gateway for on prem services to access your AWS storage services such as S3, EBS
- creates a virtual on prem file server for on prem access to S3
- can use NFS or SMB
- Cached Volume mode
	= data stored on S3
	= cache stored on site

- Stored Volume Mode
	= data stored on site
	= async back up to S3 using incremental snapshots

- Tape gateway: integration with backup software
	= create virtual tapes


Role-Based Access Control
- link policies to IAM role


Attribute-Based Access Control
- IAM IC user may have tags associated with their account
- policies associate to these tags



Resource-Based IAM Policy
- when a policy has a "Principal" key:value , means that this policy is Resource-Based
- intead of "this role can do xxx" (identity based), is "this resource can do xxx by yyy"


IAM policy
- everything is denied by default
- must give permissions
- explicit deny will always override any allow
- resource based rules are higher authority then identity based
- types of policies
	= identity based policy
	= resourced based policy
	= IAM permissions boundaries
		-> set the max permissions an identity based policy can grant an IAM principal
	= AWS Organisations service control policies (SCP)
		-> set the max permissions for an organisation or OU
	= session policy
		-> used with AssumeRole API actions

